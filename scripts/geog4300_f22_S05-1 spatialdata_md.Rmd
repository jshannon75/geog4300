---
title: "Spatial data in R"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---
### Loading and manipulating spatial data

There are several packages available to create and edit spatial data in R. This includes both raster and vector data. This script focuses on the latter. The sf (stands for "simple features") package is one efficient way to load vector data. Other popular packages for spatial data are raster, terra, and stars.


```{r, message=FALSE}
#install.packages("sf")
library(tidyverse)
library(sf)
library(tidycensus)
library(viridis)
```

The main way to read in data using sf is the function `st_read`, which uses gdal to read a variety of vector file formats (shapefile, geopackage, geojson, etc.). The resulting object is formatted just like a data frame. For example, we could read our census county dataset this way:

```{r}
cty<-st_read("data/ACSCtyData_2019ACS_simplify.gpkg")
```

For this script, we won't be using st_read. Instead, we will download public use microdata area (PUMA) boundaries for Atlanta. PUMAs have uniform populations, making them smaller than counties in metro areas but larger in rural ones. We will download boundaries using the tidycensus package and then filter for those whose name refers to Atlanta.

```{r}
atl_pumas<-get_acs(geography="puma", #Use PUMA data
                   state="GA", #Only download the state of Georgia
                   variable="B01001_001", #Get the total population variable
                   geometry=TRUE) %>% #Include the spatial boundaries
  filter(str_detect(NAME,"Atlanta")) #Filter for just Atlanta boundaries using the name

ggplot(atl_pumas) + 
  geom_sf()
```

For this analysis, we'll be looking at dollar stores in the Atlanta metro in 2020. We will read in a csv file with a national dollar store listing, select only those identified as being in Atlanta and still open (no end year). Lastly, we will use st_as_sf to convert to spatial data

```{r}
dollars<-read_csv("data/dollars.csv") %>%
  filter(str_detect(msa_name,"Atlanta") & is.na(end_year)) %>%
  st_as_sf(coords=c("x","y"), #Identify the coordinate variables
           crs=4326, #Identify the projection (4326 is WGS84)
           remove=FALSE) #Don't remove the coordinate variables
```

The sf package also has a set of spatial functions that can be used on these data. For example, you could convert the PUMA polygons to centroids. We then use geom_sf in ggplot to visualize those points.

```{r}
ggplot(atl_pumas) + geom_sf()

atl_pumas_points<-atl_pumas %>%
  st_centroid()
ggplot(atl_pumas_points) + 
  geom_sf()

#What if the color matched population?
ggplot(atl_pumas_points,aes(color=estimate)) + 
  geom_sf()
```

##Nearest neighbors

The spatialEco package has a "nni" function that allows you to easily calculate the Nearest Neighbor Index. To use it, you'll have to transform these data into a different spatial format--the as_Spatial format does this part. The NNI and z-score tell you the index and a measure of significance--generally any absolute value higher than 1.96 would be significant.

```{r}
#install.packages("spatialEco")
library(spatialEco)
nni(as_Spatial(dollars))
```

####You try it!
Create new data frames from the dollar store dataset for Family Dollar and Dollar General. Compute the NNI index for each. What does each result tell you?

```{r}

```


##Point pattern analysis with census tracts
We can use st_join to do a *spatial join* between stores and the underlying PUMAs. See [this page](https://gisgeography.com/spatial-join/) for an explanation of spatial joins. The st_join function will join all the variables for the PUMA each store is in based on its location--in this case whether the store is *within* the PUMA polygon (st_within). The tract data is not in WGS 84, so we will set the projection while joining.

```{r}
dollars_puma<-dollars %>%
  st_join(atl_pumas %>% st_transform(4326), #st_transform changes the projection to the listed EPSG number
          join=st_within) %>%
  filter(is.na(GEOID)==FALSE)

#Plot stores by tract
ggplot(dollars_puma) + 
  geom_sf(aes(color=GEOID),show.legend=FALSE)
```

We can then remove the geometry column (which contains the spatial data) using the st_set_geometry function. This will transform the stores back to a regular data frame. 

```{r}
dollars_puma_df<-dollars_puma %>%
  st_set_geometry(NULL) 
```

What if we wanted to see how many stores there are in each PUMA? We can then tally these points using the GEOID field (tract fips code) and the store category variable using the count function, which basically combines group_by and summarise.

```{r}
dollars_puma_count<-dollars_puma_df %>%
  count(GEOID,dollar_type)
```

These data are in long format. We can use pivot_wider and mutate to make this easier to read and calculate the total number of stores and the percentage of stores for each chain. 

```{r}
dollars_puma_count_wide<-dollars_puma_count %>%
  pivot_wider(names_from=dollar_type,
              values_from=n,
              values_fill=0) %>%
  mutate(total_stores=`Dollar General`+`Family Dollar`+`Dollar Tree`,
         dg_pct=`Dollar General`/total_stores*100,
         fd_pct=`Family Dollar`/total_stores*100,
         dt_pct=`Dollar Tree`/total_stores*100)
```

Now we can join those summary data to the PUMA boundary dataset. 

```{r}
atl_pumas_join<-atl_pumas %>%
  left_join(dollars_puma_count_wide)
```

Note that in order to preserve the geometry column, you always want to join non-spatial data TO spatial data. That is, it should always looks like this: spatial data %>% left_join(non-spatial data).

###Quadrats in R
Quadrat analysis is also possible in R. To create quadrats, you can use `st_make_grid` from the sf package. You can change the size and shape (hex/square).

```{r}
grid1<-st_make_grid(atl_pumas,cellsize=0.05)
grid2<-st_make_grid(atl_pumas,cellsize=0.1)
grid3<-st_make_grid(atl_pumas,cellsize=0.05,square=FALSE)

ggplot()+
  geom_sf(data=grid1)+
  geom_sf(data=atl_pumas,alpha=0.5)

ggplot()+
  geom_sf(data=grid2)+
  geom_sf(data=atl_pumas,alpha=0.5)

ggplot()+
  geom_sf(data=grid3)+
  geom_sf(data=atl_pumas,alpha=0.5)

```

To add a row number ID, just use the `row_number` function. This can be useful for spatial joins.

```{r}
grid1<-st_make_grid(atl_pumas,cellsize=0.05) %>%
  mutate(rowID=row_number())
```


###Calculating location quotient
What if we wanted the location quotient for the percentage of Dollar General stores in each PUMA to be able to more easily compare them? We already have most of the data we need to calculate this figure. First, we need to calculate a rate for the whole metro area. We can use the nrow function to count restaurants and then create an object with the percentage that are sandwich restaurants.

```{r}
total_rest=nrow(dollars_puma)
dg_rest=nrow(dollars_puma %>% filter(dollar_type=="Dollar General"))
area_ratio=dg_rest/total_rest*100
```

Getting the LQ is simple using mutate.

```{r}
atl_pumas_join<-atl_pumas_join %>%
  mutate(dg_lq=dg_pct/area_ratio)
```

#You try it!
Calculate the LQ for another chain of your choosing.

```{r}

```


### Tools for viewing/mapping spatial data
```{r}
#install.packages("tmap")
library(tmap)
```

You can make maps with ggplot, but it's not the best option out there. The tmap package is a popular tool for mapping spatial data. Here's a basic plot in tmap, which follows a similar logic to ggplot:

```{r}
tm_shape(atl_pumas_count)+
  tm_polygons()
```

You can make a choropleth map by adding a variable. 

```{r}
tm_shape(atl_pumas_count)+
  tm_polygons("dg_lq")
```

Or you can add restaurants as points. The style parameter here sets Jenks natural breaks for the data classification scheme. Notice how we also filter the dataset on the third line to show just sandwich restaurants.

```{r}
tm_shape(atl_pumas_count)+
  tm_polygons("dg_lq",style="jenks")+
tm_shape(dollars_puma %>% filter(dollar_type=="Dollar General"))+
  tm_dots(size=0.1,alpha=0.5) 
```

We can make this map prettier, adding a north arrow and scale bar and moving the legend outside.

```{r}
tm_shape(atl_pumas_count)+
  tm_polygons("dg_lq",style="jenks")+
tm_shape(dollars_puma %>% filter(dollar_type=="Dollar General"))+
  tm_dots(size=0.1,alpha=0.5) +
tm_compass()+
tm_scale_bar(position="left")+
tm_legend(legend.outside=TRUE)
```

###You try it!
Make a map showing the LQ for the dollar store chain you chose above. How does it compare to Dollar General's map?
```{r}

```

You can also make interactive maps with tmap. Make sure you set the output to the Console using the gear icon above.

```{r}
tmap_mode("view") #To shift back to static maps, use tmap_mode("plot")

tm_shape(atl_pumas_count)+
  tm_polygons("dg_pct",style="jenks",alpha=0.4)+
tm_shape(dollars_puma %>% filter(dollar_type=="Dollar General"))+
  tm_dots(size=0.1)
```

There are other good mapping packages available. For example, mapview (https://r-spatial.github.io/mapview/) provides quick interactive maps and the the leaflet package (https://rstudio.github.io/leaflet/) creates maps using that popular javascript library.

Learn more about spatial analysis in R in Manuel Gimond's web textbook: https://mgimond.github.io/Spatial/